{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T13:03:59.674514Z",
     "start_time": "2018-05-23T13:03:57.984981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'preds=predictions[:,-1,:]\\n\\npreds.shape\\n\\np1 = pd.DataFrame([int(i.split(\".\")[0].split(\"_\")[1]) for i in val_data.y_val], columns=[\\'patch_id\\'])\\np2 = pd.DataFrame(preds, columns=[i+1 for i in range(predictions.shape[2])])\\n\\np = pd.concat([p1,p2], axis=1)\\n\\np.to_csv(\"Data/Test_Predictions.csv\", sep=\",\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tifffile as tif\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from sklearn.utils import shuffle\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model, save_model\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "import pickle as pkl\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# from keras.layers import Conv2D, Merge\n",
    "# from keras.layers import Flatten, RepeatVector\n",
    "# from keras.layers import MaxPool2D \n",
    "# from keras.layers import Reshape\n",
    "# from collections import OrderedDict\n",
    "# from keras.layers import TimeDistributed\n",
    "# from keras.layers import LSTM\n",
    "# from keras.layers import Permute, Embedding\n",
    "# from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "# from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms, utils\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "os.chdir(\"../../\")\n",
    "\n",
    "class ValidationImageGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, x_metadata,batch_size, crop_size):\n",
    "        self.x = x_metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.cp = crop_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "           \n",
    "        return np.array([tif.imread(file_name)[:,self.cp:-self.cp,self.cp:-self.cp] \n",
    "                         for file_name in batch_x])\n",
    "\n",
    "class Val_load():\n",
    "    \n",
    "    def init_load(self, root_dir):\n",
    "        self.path = root_dir\n",
    "    \n",
    "    def validation_data_loading(self,root_dir):\n",
    "        self.x_val=[]\n",
    "        self.y_val=[]\n",
    "        for filename in os.listdir(root_dir):\n",
    "            for image in os.listdir(root_dir+'/'+str(filename)):\n",
    "                self.x_val.extend([root_dir+'/'+str(filename)+'/'+str(image)])\n",
    "                self.y_val.append(image)\n",
    "\n",
    "val_data = Val_load()\n",
    "val_data.validation_data_loading(root_dir='patchTest')\n",
    "val_data.x_val=np.array(val_data.x_val)\n",
    "\n",
    "#Total test images\n",
    "\n",
    "#if it returns nx5x10\n",
    "'''preds=predictions[:,-1,:]\n",
    "\n",
    "preds.shape\n",
    "\n",
    "p1 = pd.DataFrame([int(i.split(\".\")[0].split(\"_\")[1]) for i in val_data.y_val], columns=['patch_id'])\n",
    "p2 = pd.DataFrame(preds, columns=[i+1 for i in range(predictions.shape[2])])\n",
    "\n",
    "p = pd.concat([p1,p2], axis=1)\n",
    "\n",
    "p.to_csv(\"Data/Test_Predictions.csv\", sep=\",\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T13:04:13.548984Z",
     "start_time": "2018-05-23T13:04:07.269324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72849,)\n"
     ]
    }
   ],
   "source": [
    "print(val_data.x_val.shape)\n",
    "validation_x=ValidationImageGenerator(val_data.x_val,32,32)\n",
    "classifier = load_model(\"Code/Models/RCNN_ResNext1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=classifier.predict_generator(validation_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-23T12:59:18.741389Z",
     "start_time": "2018-05-23T12:59:18.332721Z"
    }
   },
   "outputs": [],
   "source": [
    "p = pd.read_csv(\"Data/Test_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:54:18.489338Z",
     "start_time": "2018-05-06T23:54:17.463703Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def prediction_to_species(p, max_ranks=5):\n",
    "    \"\"\"\n",
    "    Function that will take in the prediction of the CNN-LSTM model and get the species id.\n",
    "\n",
    "    Inputs :\n",
    "    ypred : Vector of dimensions n x 10.\n",
    "\n",
    "    Outputs :\n",
    "    DataFrame of type :\n",
    "    patch_id (int), species_glc_id(int) , probability(float) , rank(int) .\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    e2 = pkl.load(open(\"Data/Embed2.pkl\", \"rb\"))\n",
    "    #print (\"Size of the species embedding pickle : \", e2.shape)\n",
    "    ypred = p.values[:,2:]\n",
    "    # Species embeddings from indices 0 to 3335, of shape 3336.\n",
    "    species_embedding = e2['species_glc_id'][-3336:, :]\n",
    "    df = pd.read_csv(\"occurrences_train.csv\", low_memory=False)\n",
    "    df = df[[\"class\", \"order\", \"family\", \"genus\", \"species_glc_id\"]]\n",
    "    unique_species_id = df['species_glc_id'].unique()\n",
    "    masterdf = pd.DataFrame()\n",
    "\n",
    "    cntr = 0\n",
    "    for species in ypred:\n",
    "        \n",
    "        if(cntr % 1000 == 0):\n",
    "            print(cntr)\n",
    "        # Now compare this vector with each vector in species_embedding.\n",
    "        '''\n",
    "        GET THE PATCH FROM THE NAME OF THE IMAGES.\n",
    "        patch_id is a list of patch names for the test images.\n",
    "        '''\n",
    "        patch_id = np.array([p.values[cntr,1]]*max_ranks).reshape(-1,1)\n",
    "        cntr+=1\n",
    "        embedding_distances = euclidean_distances(species_embedding, species.reshape(-1,10))\n",
    "        top_indices = embedding_distances.flatten().argsort()[:max_ranks]\n",
    "        top_distances = embedding_distances[top_indices].tolist()\n",
    "        top_distances = softmax(np.array([1.0/i[0] for i in top_distances])).reshape(-1,1)\n",
    "        rank = np.array(range(1,len(top_indices)+1)).reshape(-1,1)\n",
    "        # Now get the species_glc_id , from the numpy array of unique species.\n",
    "\n",
    "        # Use the top_indices, to get the species_glc_id from unique_species_id.\n",
    "        species_glc_id = unique_species_id[top_indices].reshape(-1,1)\n",
    "        masterdf = pd.concat([masterdf, pd.DataFrame(\n",
    "            np.concatenate((patch_id, species_glc_id, top_distances, rank), axis=1))], axis=0)\n",
    "\n",
    "    #Writing the dataframe to a csv.\n",
    "    masterdf.columns = [\"patch_id\",\"species_glc_id\",\"probability\",\"rank\"]\n",
    "    masterdf[\"patch_id\"] = masterdf[\"patch_id\"].astype(int)\n",
    "    masterdf[\"species_glc_id\"] = masterdf[\"species_glc_id\"].astype(int)\n",
    "    masterdf[\"probability\"] = masterdf[\"probability\"].astype(float)\n",
    "    masterdf[\"rank\"] = masterdf[\"rank\"].astype(int)\n",
    "    return masterdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:56:29.205458Z",
     "start_time": "2018-05-06T23:56:27.164935Z"
    }
   },
   "outputs": [],
   "source": [
    "num_ranks = int(input(\"How many ranks for each image : \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:57:53.022541Z",
     "start_time": "2018-05-06T23:56:30.268431Z"
    }
   },
   "outputs": [],
   "source": [
    "mpd = pd.DataFrame()\n",
    "for i in range(0,p.shape[0],20000):\n",
    "    print(\"Main Iter : \", i/20000)\n",
    "    if(i==0):\n",
    "        mpd = prediction_to_species(p, num_ranks)\n",
    "    else:\n",
    "        mpd = pd.concat([mpd, prediction_to_species(p, num_ranks)], axis=0)\n",
    "mpd.to_csv(\"MLRG_SSN3_run100.csv\",sep=\";\",index=False,header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T01:16:00.718359Z",
     "start_time": "2018-04-25T01:15:44.315937Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tifffile as tif\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from multiprocessing import Pool\n",
    "from keras.utils import Sequence\n",
    "from collections import OrderedDict\n",
    "from keras.initializers import he_normal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
    "from keras.models import Sequential, Model, load_model, save_model \n",
    "from keras.layers import Dropout, Activation, Dense, Conv2D, Merge, Flatten, RepeatVector, MaxPool2D, Input\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T01:19:53.565836Z",
     "start_time": "2018-04-25T01:19:53.525284Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, x_metadata, y_metadata, batch_size, crop_size):\n",
    "        self.x = x_metadata\n",
    "        self.y = y_metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.cp = crop_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        y1, y2, y3 = [], [], []\n",
    "        \n",
    "        for t in batch_y:\n",
    "            y1.append(t[0])\n",
    "            y2.append(t[1])\n",
    "            y3.append(t[2])\n",
    "           \n",
    "        return [np.array([np.transpose(tif.imread(file_name)/255.0,(1,2,0))[self.cp:-self.cp,self.cp:-self.cp,:] \n",
    "                          for file_name in batch_x])], [np.array(y1),np.array(y2),np.array(y3)]         \n",
    "\n",
    "class CNN_Model:\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        \n",
    "        self.path = directory\n",
    "        \n",
    "        df = pd.read_csv(\"occurrences_train.csv\",low_memory=False)\n",
    "        with open(\"Data/hierarchy_data.pkl\",\"rb\") as f:\n",
    "            hd = pkl.load(f)\n",
    "        with open(\"Data/class_encoding.pkl\",\"rb\") as f:\n",
    "            self.classes = pkl.load(f)\n",
    "        with open(\"Data/order_encoding.pkl\",\"rb\") as f:\n",
    "            self.orders = pkl.load(f)\n",
    "        with open(\"Data/family_encoding.pkl\",\"rb\") as f:\n",
    "            self.families = pkl.load(f)\n",
    "        with open(\"Data/genus_encoding.pkl\",\"rb\") as f:\n",
    "            self.genuses = pkl.load(f)\n",
    "        with open(\"Data/specie_encoding.pkl\",\"rb\") as f:\n",
    "            self.species = pkl.load(f)\n",
    "\n",
    "        self.train_pathdata_x = []\n",
    "        self.train_seq_y = []\n",
    "        self.test_pathdata_x = []\n",
    "        self.test_seq_y = []\n",
    "        \n",
    "        print(\"Getting paths to images\")\n",
    "        for cls in hd.keys():\n",
    "            print(cls)\n",
    "            for order in hd[cls].keys():\n",
    "                for family in hd[cls][order].keys():\n",
    "                    for genus in hd[cls][order][family].keys():\n",
    "                        for specie in hd[cls][order][family][genus]:\n",
    "                            for im in os.listdir(self.path+\"train/\"+str(self.classes[cls])+\"/\"+str(self.orders[order])\n",
    "                                                 +\"/\"+str(self.families[family])+\"/\"+str(self.genuses[genus])+\"/\"+str(specie)):\n",
    "                                self.train_pathdata_x.append(self.path+\"train/\"+str(self.classes[cls])+\"/\"+str(self.orders[order])\n",
    "                                                             +\"/\"+str(self.families[family])+\"/\"+str(self.genuses[genus])+\"/\"+str(specie)+\"/\"\n",
    "                                                             +im)\n",
    "                                \n",
    "                            for im in os.listdir(self.path+\"test/\"+str(self.classes[cls])+\"/\"+str(self.orders[order])\n",
    "                                                 +\"/\"+str(self.families[family])+\"/\"+str(self.genuses[genus])+\"/\"+str(specie)):\n",
    "                                self.test_pathdata_x.append(self.path+\"test/\"+str(self.classes[cls])+\"/\"+str(self.orders[order])\n",
    "                                                             +\"/\"+str(self.families[family])+\"/\"+str(self.genuses[genus])+\"/\"+str(specie)+\"/\"\n",
    "                                                             +im)\n",
    "        \n",
    "        print(\"Done getting paths\")\n",
    "        np.random.shuffle(self.train_pathdata_x)\n",
    "        np.random.shuffle(self.test_pathdata_x)\n",
    "        \n",
    "        print(\"Getting labels for images\")\n",
    "        for p in self.train_pathdata_x:\n",
    "            y = p.split(\"/\")\n",
    "            c = int(y[3])\n",
    "            o = int(y[4])\n",
    "            f = int(y[5])\n",
    "            g = int(y[6])\n",
    "            s = int(y[7])\n",
    "            z1 = np.zeros(len(self.classes.keys()))\n",
    "            z1[c] = 1.0\n",
    "            z2 = np.zeros(len(self.orders.keys()))\n",
    "            z2[o] = 1.0\n",
    "            z3 = np.zeros(len(self.species))\n",
    "            z3[s-1] = 1.0\n",
    "            self.train_seq_y.append([z1,z2,z3])\n",
    "            \n",
    "        for p in self.test_pathdata_x:\n",
    "            y = p.split(\"/\")\n",
    "            c = int(y[3])\n",
    "            o = int(y[4])\n",
    "            f = int(y[5])\n",
    "            g = int(y[6])\n",
    "            s = int(y[7])\n",
    "            z1 = np.zeros(len(self.classes.keys()))\n",
    "            z1[c] = 1.0\n",
    "            z2 = np.zeros(len(self.orders.keys()))\n",
    "            z2[o] = 1.0\n",
    "            z3 = np.zeros(len(self.species))\n",
    "            z3[s-1] = 1.0\n",
    "            self.test_seq_y.append([z1,z2,z3])\n",
    "        print(\"Done getting labels\")\n",
    "            \n",
    "    def model_create(self, batch_size=32):\n",
    "        \n",
    "        alpha = K.variable(value=0.98, dtype=\"float32\", name=\"alpha\") # A1 in paper\n",
    "        beta = K.variable(value=0.01, dtype=\"float32\", name=\"beta\") # A2 in paper\n",
    "        gamma = K.variable(value=0.01, dtype=\"float32\", name=\"gamma\") # A3 in paper\n",
    "\n",
    "        img_input = Input(shape=(32,32,33), name='input')\n",
    "        #--- block 1 ---\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPool2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "        #--- block 2 ---\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPool2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "        #--- coarse 1 branch ---\n",
    "        c_1_bch = Flatten(name='c1_flatten')(x)\n",
    "        c_1_bch = Dense(256, activation='relu', name='c1_fc_cifar10_1')(c_1_bch)\n",
    "        c_1_bch = BatchNormalization()(c_1_bch)\n",
    "        c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "        c_1_bch = Dense(256, activation='relu', name='c1_fc2')(c_1_bch)\n",
    "        c_1_bch = BatchNormalization()(c_1_bch)\n",
    "        c_1_bch = Dropout(0.5)(c_1_bch)\n",
    "        c_1_pred = Dense(len(self.classes.keys()), activation='softmax', name='class')(c_1_bch)\n",
    "\n",
    "        #--- block 3 ---\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPool2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "        #--- coarse 2 branch ---\n",
    "        c_2_bch = Flatten(name='c2_flatten')(x)\n",
    "        c_2_bch = Dense(1024, activation='relu', name='c2_fc_cifar100_1')(c_2_bch)\n",
    "        c_2_bch = BatchNormalization()(c_2_bch)\n",
    "        c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "        c_2_bch = Dense(1024, activation='relu', name='c2_fc2')(c_2_bch)\n",
    "        c_2_bch = BatchNormalization()(c_2_bch)\n",
    "        c_2_bch = Dropout(0.5)(c_2_bch)\n",
    "        c_2_pred = Dense(len(self.orders.keys()), activation='softmax', name='order')(c_2_bch)\n",
    "\n",
    "        #--- block 4 ---\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPool2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "\n",
    "        #--- block 5 ---\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "\n",
    "        #--- fine block ---\n",
    "        x = Flatten(name='flatten')(x)\n",
    "        x = Dense(4096, activation='relu', name='fc_cifar100_1')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(4096, activation='relu', name='fc_cifar100_2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        fine_pred = Dense(len(self.species), activation='softmax', name='species')(x)\n",
    "\n",
    "        model = Model(img_input, [c_1_pred, c_2_pred, fine_pred], name='IMAGECLEF18 hierarchy')\n",
    "        \n",
    "        #----------------------- compile and fit ---------------------------\n",
    "        sgd = optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer=sgd, \n",
    "                      loss_weights=[alpha, beta, gamma], \n",
    "                      # optimizer=keras.optimizers.Adadelta(),\n",
    "                      metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def fit_generator(self, num_epochs=10, batch_size=32, crop_size=16):        \n",
    "        try:\n",
    "            classifier = load_model(\"Code/Models/CNN-RNN_1.h5\")\n",
    "        except:\n",
    "            classifier = self.model_create(batch_size=batch_size)\n",
    "            train_data = ImageDataGenerator(self.train_pathdata_x, self.train_seq_y, batch_size, crop_size)\n",
    "            print(\"Training\")\n",
    "            history = classifier.fit_generator(train_data, epochs=num_epochs, use_multiprocessing=True,shuffle=True)\n",
    "            classifier.save(\"Code/Models/CNN-RNN_1.h5\")\n",
    "        print(\"Testing\")\n",
    "        test_data = ImageDataGenerator(self.test_pathdata_x, self.test_seq_y, batch_size, crop_size)\n",
    "        scores = classifier.evaluate_generator(test_data, use_multiprocessing=True)\n",
    "        print(\"Loss : \", scores[0])\n",
    "        print(\"Accuracy : \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-25T01:19:54.202Z"
    }
   },
   "outputs": [],
   "source": [
    "ob = CNN_Model(\"Data/Hierarchial Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-24T17:50:38.358Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ob.fit_generator(num_epochs=10, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

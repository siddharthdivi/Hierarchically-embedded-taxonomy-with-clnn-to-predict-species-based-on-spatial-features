{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T03:14:00.003481Z",
     "start_time": "2018-05-25T03:13:59.959818Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tifffile as tif\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from sklearn.utils import shuffle\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model, save_model\n",
    "#from livelossplot import PlotLossesKeras\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# from torchvision import transforms, utils\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:19:57.065140Z",
     "start_time": "2018-05-20T16:19:57.057626Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, x_metadata, y_metadata, batch_size, crop_size):\n",
    "        self.x = x_metadata\n",
    "        self.y = y_metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.cp = crop_size\n",
    "        #self.dic = {0:3,1:2,2:2,3:1,4:3,5:3,6:1,7:1,8:1,9:1}\n",
    "        #self.dic = {0:4,1:3,2:3,3:5,4:3,5:2,6:5,7:4,8:2,9:5}\n",
    "        self.conv_dic = {0:[133,1176],1:[-10.00984,18.36730],2:[7.846126,20.94560],3:[41.182110,59.95573],4:[302.772980,777.74048],5:[6.182446,36.54550],6:[-28.248663,5.33183],7:[16.744829,41.94211],8:[-14.122952,22.96798],9:[-17.672335,26.44534],10:[-2.738379,26.44534],11:[-17.672335,11.73241],12:[318.297485,2543.30225],13:[43.063732,285.43790],14:[3.022581,135.58406],15:[8.283675,57.78888],16:[121.616867,855.52594],17:[19.868601,421.27750],18:[19.868601,851.60620],19:[60.590000,520.31244],20:[-187.999999,4672.000000]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        x1 = []\n",
    "        x2 = []\n",
    "        x3 = []\n",
    "        feat=[]\n",
    "        for i in range(len(batch_x)):\n",
    "            #x.append(np.transpose(tif.imread(batch_x[i])[:21,self.cp:-self.cp,self.cp:-self.cp],(1,2,0)))\n",
    "            temp = tif.imread(batch_x[i])[:3,:,:]\n",
    "            for k in range(3):\n",
    "                temp[k] = self.conv_dic[k][0] + (self.conv_dic[k][1] - self.conv_dic[k][0]) * ((temp[k]/255.0) - 0.1) / 0.8\n",
    "            x1.append(np.transpose(temp[0],(1,2,0)))\n",
    "            x2.append(np.transpose(temp[1],(1,2,0)))\n",
    "            x3.append(np.transpose(temp[2],(1,2,0)))\n",
    "            #x.append(np.transpose(tif.imread(batch_x[i]),(1,2,0)))\n",
    "        return [np.array(x1),np.array(x2),np.array(x3)], np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:19:57.164740Z",
     "start_time": "2018-05-20T16:19:57.067195Z"
    }
   },
   "outputs": [],
   "source": [
    "class Data_Preprocess():\n",
    "    \n",
    "    def init_load(self, root_dir, csv_file):\n",
    "        self.df = pd.read_csv(csv_file, low_memory=False)\n",
    "        self.path = root_dir\n",
    "    \n",
    "    def create_mappings_for_unique_labels(self):\n",
    "        # getting all unique names from csv file\n",
    "        self.classes = list(sorted(self.df['class'].unique()))\n",
    "        self.orders = list(sorted(self.df['order'].unique()))\n",
    "        self.family = list(sorted(self.df['family'].unique()))\n",
    "        self.genus = list(sorted(self.df['genus'].unique()))\n",
    "        self.species = list(sorted(self.df['species_glc_id'].unique()))\n",
    "        self.all_names = self.classes + self.orders + self.family + self.genus + self.species\n",
    "        # creting map for one hot encoding / embedding\n",
    "        self.all_encoded = {}\n",
    "        self.all_rev_encoded = {}\n",
    "        \n",
    "        for i, name in enumerate(self.all_names):\n",
    "            self.all_encoded[str(name)] = i\n",
    "            self.all_rev_encoded[int(i)] = str(name)\n",
    "        \n",
    "    # embedding all the names\n",
    "    def create_embedding(self):\n",
    "        print(\"Done\")\n",
    "        columns = ['class','order','family','genus','species_glc_id']\n",
    "        self.df = pd.DataFrame(shuffle(self.df.values), columns=self.df.columns)\n",
    "        try:\n",
    "            self.embed_vectors1 = pkl.load(open(\"Data/Embed1.pkl\",\"rb\"))\n",
    "        except:\n",
    "            self.embed_vectors1 = {}\n",
    "            for col_idx in range(len(columns)-1):\n",
    "                x,y = [],[]\n",
    "                print(\"Collecting \" + columns[col_idx] + \",\" + columns[col_idx+1])\n",
    "                x.extend([self.all_encoded[str(i)] for i in self.df[columns[col_idx]]])\n",
    "                y.extend([self.all_encoded[str(i)] for i in self.df[columns[col_idx+1]]])\n",
    "                x,y = np.array(x), np.array(y)\n",
    "                print(x.shape, y.shape)\n",
    "                print(np.max(x))\n",
    "                model = Sequential()\n",
    "                model.add(Embedding(input_dim=np.max(x)+1, output_dim=10, input_length=1, name=\"Embed\"))\n",
    "                model.add(Flatten())\n",
    "                model.add(Dense(1, activation='relu'))\n",
    "                model.compile(optimizer='nadam',loss='logcosh', metrics=['mae','accuracy'])\n",
    "                model.summary()\n",
    "                model.fit(x,y,epochs=30,batch_size=100)\n",
    "                self.embed_vectors1[columns[col_idx]] = np.array(model.get_layer(\"Embed\").get_weights()[0])\n",
    "                del model\n",
    "            pkl.dump(self.embed_vectors1, open(\"Data/Embed1.pkl\",\"wb\"))\n",
    "        \n",
    "        try:\n",
    "            self.embed_vectors2 = pkl.load(open(\"Data/Embed2.pkl\",\"rb\"))\n",
    "        except:\n",
    "            self.embed_vectors2 = {}\n",
    "            x,y = [self.all_encoded[str(i)] for i in self.df[columns[-1]]], [self.embed_vectors1[columns[-2]][self.all_encoded[str(i)]] for i in self.df[columns[-2]]]\n",
    "            x,y = np.array(x), np.array(y)\n",
    "            print(x.shape, y.shape)\n",
    "            model = Sequential()\n",
    "            model.add(Embedding(input_dim=max(x)+1, output_dim=10, input_length=1, name=\"Embed\"))\n",
    "            model.add(Flatten())\n",
    "            model.add(Dense(10))\n",
    "            model.compile(optimizer='nadam',loss='logcosh', metrics=['mae','accuracy'])\n",
    "            model.summary()\n",
    "            model.fit(x,y,epochs=50,batch_size=200)\n",
    "            self.embed_vectors2[columns[-1]] = np.array(model.get_layer(\"Embed\").get_weights()[0])\n",
    "            del model\n",
    "            pkl.dump(self.embed_vectors2,open(\"Data/Embed2.pkl\",\"wb\"))\n",
    "        \n",
    "    def train_test_data_loading(self):\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = [], [], [], []\n",
    "        for cls in self.df['class'].unique():\n",
    "            #if(cls not in ['Magnoliopsida']):\n",
    "                for order in self.df[self.df['class']==cls]['order'].unique():\n",
    "                    for family in self.df[(self.df['class']==cls) & (self.df['order']==order)]['family'].unique():\n",
    "                        for genus in self.df[(self.df['class']==cls) & (self.df['order']==order) & (self.df['family']==family)]['genus'].unique():\n",
    "                            for species in self.df[(self.df['class']==cls) & (self.df['order']==order) & (self.df['family']==family) & (self.df['genus']==genus)]['species_glc_id'].unique():\n",
    "                                path = self.path+\"train/\"+cls+\"/\"+order+\"/\"+family+\"/\"+genus+\"/\"+str(species)+\"/\"\n",
    "                                self.x_train.extend([path+i for i in os.listdir(path)])\n",
    "                                path = self.path+\"test/\"+cls+\"/\"+order+\"/\"+family+\"/\"+genus+\"/\"+str(species)+\"/\"\n",
    "                                self.x_test.extend([path+i for i in os.listdir(path)])\n",
    "        \n",
    "        np.random.shuffle(self.x_train)\n",
    "        np.random.shuffle(self.x_test)\n",
    "        \n",
    "        for im in self.x_train:\n",
    "            l = im.split(\"/\")\n",
    "            c, o, f, g, s = self.all_encoded[l[3]], self.all_encoded[l[4]], self.all_encoded[l[5]], self.all_encoded[l[6]], self.all_encoded[l[7]]\n",
    "            self.y_train.append([[c],[o],[f],[g],[s]])\n",
    "            \n",
    "        for im in self.x_test:\n",
    "            l = im.split(\"/\")\n",
    "            c, o, f, g, s = self.all_encoded[l[3]], self.all_encoded[l[4]], self.all_encoded[l[5]], self.all_encoded[l[6]], self.all_encoded[l[7]] #self.embed_vectors1['class'][self.all_encoded[l[3]]], self.embed_vectors1['order'][self.all_encoded[l[4]]], self.embed_vectors1['family'][self.all_encoded[l[5]]], self.embed_vectors1['genus'][self.all_encoded[l[6]]], self.embed_vectors2['species_glc_id'][int(l[7])]\n",
    "            self.y_test.append([[c],[o],[f],[g],[s]])\n",
    "        \n",
    "    def ordered_call(self, root_dir, csv_file):\n",
    "        print(\"Creating the data preprocessing object and loading csv\")\n",
    "        self.init_load(root_dir, csv_file)\n",
    "        print(\"Done!\")\n",
    "        print(\"Creating unique mappings for labels\")\n",
    "        self.create_mappings_for_unique_labels()\n",
    "        #print(\"Done!\")\n",
    "        #print(\"Creating embeddings for all the names\")\n",
    "        #self.create_embedding()\n",
    "        print(\"Done!\")\n",
    "        print(\"Loading test and train image paths and corresponding labels\")\n",
    "        self.train_test_data_loading()\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:19:57.262825Z",
     "start_time": "2018-05-20T16:19:57.167596Z"
    }
   },
   "outputs": [],
   "source": [
    "data = Data_Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:21:15.455470Z",
     "start_time": "2018-05-20T16:19:57.265385Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the data preprocessing object and loading csv\n",
      "Done!\n",
      "Creating unique mappings for labels\n",
      "Done!\n",
      "Loading test and train image paths and corresponding labels\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data.ordered_call(root_dir=\"Data/Hierarchial Data/\", csv_file=\"occurrences_train.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T15:42:37.410956Z",
     "start_time": "2018-05-20T15:42:36.345131Z"
    }
   },
   "source": [
    "p1 = pd.DataFrame(np.concatenate((data.y_train, data.y_test), axis=0))\n",
    "\n",
    "p1 = p1.iloc[:10000,:]\n",
    "\n",
    "p1.to_csv(\"values.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "n1, n2 = [], []\n",
    "for i in data.y_train:\n",
    "    l = []\n",
    "    for j in i:\n",
    "        l.append(data.all_rev_encoded[j])\n",
    "    n1.append(l)\n",
    "n1 = np.array(n1)\n",
    "for i in data.y_test:\n",
    "    l = []\n",
    "    for j in i:\n",
    "        l.append(data.all_rev_encoded[j])\n",
    "    n2.append(l)\n",
    "n2 = np.array(n2)\n",
    "\n",
    "p2 = pd.DataFrame(np.concatenate((np.array([\"class\",\"order\",\"family\",\"genus\",\"species\"]).reshape(1,5),n1,n2), axis=0))\n",
    "\n",
    "p2 = p2.iloc[:10001,:]\n",
    "\n",
    "p2.to_csv(\"words.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T15:42:37.485445Z",
     "start_time": "2018-05-20T15:42:37.413120Z"
    }
   },
   "source": [
    "p = pd.DataFrame(np.concatenate((data.embed_vectors1['class'], data.embed_vectors1['order'][-len(data.orders):], \n",
    "                                 data.embed_vectors1['family'][-len(data.family):], data.embed_vectors1['genus'][-len(data.genus):], \n",
    "                                 data.embed_vectors2['species_glc_id'][-len(data.species):]), axis=0))\n",
    "\n",
    "p.to_csv(\"embeddings.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "names = pd.DataFrame(np.array(list(data.all_encoded.keys())))\n",
    "\n",
    "names.to_csv(\"names.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:21:16.087072Z",
     "start_time": "2018-05-20T16:21:15.457953Z"
    }
   },
   "outputs": [],
   "source": [
    "#data.y_train, data.y_test = np.array(data.y_train).reshape(-1,1), np.array(data.y_test).reshape(-1,1)\n",
    "data.x_train, data.y_train, data.x_test, data.y_test = np.array(data.x_train), np.array(data.y_train), np.array(data.x_test), np.array(data.y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:21:16.106737Z",
     "start_time": "2018-05-20T16:21:16.089615Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN_Model:\n",
    "    \n",
    "    def __init__(self, data_object):\n",
    "        self.img_height = 64\n",
    "        self.img_width = 64\n",
    "        self.img_channels = 1\n",
    "        self.cardinality = 32\n",
    "        self.data_object = data_object\n",
    "        self.num_classes = len(data_object.all_encoded.keys())\n",
    "        \n",
    "    def three_cnn(self, x1,x2,x3):\n",
    "        \n",
    "        y = layers.Conv2D(filters=96, kernel_size=3, activation='relu')(x1)\n",
    "        y = layers.MaxPool2D(2)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=256, kernel_size=5, activation='relu')(y)\n",
    "        y = layers.MaxPool2D(3)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=256, kernel_size=7, activation='relu')(y)\n",
    "        y = layers.MaxPool2D(3)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        l1 = layers.Flatten()(y)\n",
    "        \n",
    "        y = layers.Conv2D(filters=96, kernel_size=3, activation='relu')(x2)\n",
    "        y = layers.MaxPool2D(2)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=256, kernel_size=5, activation='relu')(y)\n",
    "        y = layers.MaxPool2D(3)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=256, kernel_size=7, activation='relu')(y)\n",
    "        y = layers.MaxPool2D(3)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        l2 = layers.Flatten()(y)\n",
    "        \n",
    "        y = layers.Conv2D(filters=96, kernel_size=3, activation='relu')(x3)\n",
    "        y = layers.MaxPool2D(2)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=256, kernel_size=5, activation='relu')(y)\n",
    "        y = layers.MaxPool2D(3)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        y = layers.Conv2D(filters=256, kernel_size=7, activation='relu')(y)\n",
    "        y = layers.MaxPool2D(3)(y)\n",
    "        y = layers.BatchNormalization()(y)\n",
    "        l3 = layers.Flatten()(y)\n",
    "        \n",
    "        merg=layers.Concatenate()([l1,l2,l3])\n",
    "        \n",
    "        x = layers.RepeatVector(5)(merg)\n",
    "        x = layers.LSTM(128, return_sequences=True)(x)\n",
    "        x = layers.LSTM(64, return_sequences=True)(x)\n",
    "        x = layers.TimeDistributed(layers.Dense(128, activation='relu'))(x)\n",
    "        x = layers.TimeDistributed(layers.Dense(self.num_classes, activation='softmax'))(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    def model_create(self, time_steps, batch_size):\n",
    "        image_tensor1 = layers.Input(shape=(self.img_height, self.img_width, self.img_channels))\n",
    "        image_tensor2 = layers.Input(shape=(self.img_height, self.img_width, self.img_channels))\n",
    "        image_tensor3 = layers.Input(shape=(self.img_height, self.img_width, self.img_channels))\n",
    "        network_output = self.three_cnn(image_tensor1,image_tensor2,image_tensor3)  \n",
    "        model = models.Model(inputs=[image_tensor1,image_tensor2,image_tensor3], outputs=[network_output])\n",
    "        print(model.summary())\n",
    "        # Compiling the CNN\n",
    "        model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy','mae'])\n",
    "        return model\n",
    "    \n",
    "    def fit_generator(self, num_epochs=10, batch_size=32, crop_size=16, time_steps=5):        \n",
    "        try:\n",
    "            classifier = load_model(\"Code/Models/RCNN_ResNext1.h5\")\n",
    "        except:\n",
    "            print(\"Training\")\n",
    "            classifier = self.model_create(time_steps=time_steps, batch_size=batch_size)\n",
    "            train_data = ImageDataGenerator(self.data_object.x_train, self.data_object.y_train, batch_size, crop_size)\n",
    "            #test_data = ImageDataGenerator(self.data_object.x_test, self.data_object.y_test, batch_size, crop_size)\n",
    "            history = classifier.fit_generator(train_data, epochs=num_epochs, use_multiprocessing=True,shuffle=True)\n",
    "            return classifier\n",
    "            #Error saving the file.\n",
    "            classifier.save(\"Code/Models/RCNN_ResNext1.h5\")\n",
    "        print(\"Testing\")\n",
    "        test_data = ImageDataGenerator(self.data_object.x_test, self.data_object.y_test, batch_size, crop_size)\n",
    "        scores = classifier.evaluate_generator(test_data, use_multiprocessing=True)\n",
    "        print(\"Loss : \", scores[0])\n",
    "        print(\"Metrics : \", scores[1:])\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:21:16.220301Z",
     "start_time": "2018-05-20T16:21:16.108988Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data_Preprocess' object has no attribute 'all_encoded'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-faba489f974b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1264eaccb589>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_object)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbasic_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Data_Preprocess' object has no attribute 'all_encoded'"
     ]
    }
   ],
   "source": [
    "model_object = CNN_Model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-20T16:28:57.788354Z",
     "start_time": "2018-05-20T16:21:16.223834Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64, 21)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 62, 62, 96)   18240       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 31, 31, 96)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 31, 31, 96)   384         max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 27, 27, 256)  614656      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 9, 9, 256)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 9, 9, 256)    1024        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 3, 3, 256)    3211520     batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 256)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 1, 256)    1024        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 59)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 256)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           1920        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 288)          0           flatten_1[0][0]                  \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 5, 288)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 5, 128)       213504      repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 5, 64)        49408       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 5, 128)       8320        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 5, 4567)      589143      time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 4,709,143\n",
      "Trainable params: 4,707,927\n",
      "Non-trainable params: 1,216\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n",
      "4781/4781 [==============================] - 1932s 404ms/step - loss: 4.0218 - acc: 0.2449 - mean_absolute_error: 611.6024\n"
     ]
    }
   ],
   "source": [
    "classifier = model_object.fit_generator(num_epochs=1, batch_size=32, crop_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = ImageDataGenerator(data.x_test, data.y_test,32,32)\n",
    "scores = classifier.evaluate_generator(test_data, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " classifier.save(\"Code/Models/RCNN_ResNext1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Try new metric"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.222521Z",
     "start_time": "2018-05-05T16:01:08.364Z"
    },
    "hidden": true
   },
   "source": [
    "classifier = load_model(\"Code/Models/RCNN_ResNext.h5\")\n",
    "test_data = ImageDataGenerator(data.x_test[:10000], data.y_test[:10000], 32, 16)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.224051Z",
     "start_time": "2018-05-05T16:01:08.366Z"
    },
    "hidden": true
   },
   "source": [
    "predictions = classifier.predict_generator(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.226247Z",
     "start_time": "2018-05-05T16:01:08.368Z"
    },
    "hidden": true
   },
   "source": [
    "species_embedding = data.embed_vectors2['species_glc_id']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.228306Z",
     "start_time": "2018-05-05T16:01:08.370Z"
    },
    "hidden": true
   },
   "source": [
    "y_test = np.array(data.y_test)[:,-1,:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.230005Z",
     "start_time": "2018-05-05T16:01:08.372Z"
    },
    "hidden": true
   },
   "source": [
    "preds = predictions[:,-1,:]\n",
    "trues = [np.flatnonzero((species_embedding == i).all(1)) for i in y_test]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.231604Z",
     "start_time": "2018-05-05T16:01:08.374Z"
    },
    "hidden": true
   },
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.233281Z",
     "start_time": "2018-05-05T16:01:08.378Z"
    },
    "hidden": true
   },
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def prediction_to_species(ypred, unique_species_id, max_ranks=5):\n",
    "    e2 = pkl.load(open(\"Data/Embed2.pkl\", \"rb\"))\n",
    "    species_embedding = e2['species_glc_id'][-3336:]\n",
    "\n",
    "    embedding_distance = euclidean_distances(species_embedding, ypred.reshape(-1,10))\n",
    "    top_indices = embedding_distance.flatten().argsort()[:max_ranks]\n",
    "    species_glc_id = pd.Series(unique_species_id)[top_indices].reshape(-1,1)\n",
    "    return species_glc_id"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-05T16:06:40.235358Z",
     "start_time": "2018-05-05T16:01:08.380Z"
    },
    "hidden": true
   },
   "source": [
    "s = 0\n",
    "for i in range(len(trues)):\n",
    "    if(i%100 == 1): print(i, s/i)\n",
    "    try:\n",
    "        ypred = prediction_to_species(preds[i],data.species, 100)\n",
    "        r = ypred.index(trues[i])\n",
    "    except:\n",
    "        r = 3336\n",
    "    s = s+(1.0/r)\n",
    "print(s/i)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWxsjuHbwKjp"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T02:45:56.420106Z",
     "start_time": "2018-12-23T02:45:51.352823Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#This code is for adaptive GPU usage\n",
    "import keras.backend as K\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T18:48:04.399882Z",
     "start_time": "2018-12-21T18:48:04.395632Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "7ggzsduTxQxH",
    "outputId": "60e58f68-0ddf-45c0-f776-f5fe47dc5db8"
   },
   "outputs": [],
   "source": [
    "#!pip3 install cntk-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T04:28:36.289648Z",
     "start_time": "2018-12-22T04:27:13.676431Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "Y5zp27Y7xVT1",
    "outputId": "a864dbed-26b1-499a-eb48-f9438b9ce1e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for divi: \n"
     ]
    }
   ],
   "source": [
    "#!sudo pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T18:48:16.509716Z",
     "start_time": "2018-12-21T18:48:16.505825Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "_QhSEjEawRE-",
    "outputId": "fcef4d07-201e-43db-d40c-85b988a135b1"
   },
   "outputs": [],
   "source": [
    "#drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T18:48:16.629481Z",
     "start_time": "2018-12-21T18:48:16.512948Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "nRwqbzoDwUm3",
    "outputId": "2904e956-cb7e-4201-ee09-5254aaccef5a"
   },
   "outputs": [],
   "source": [
    "#!ls 'gdrive/My Drive/Microsoft AI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WOmEaksFwuHJ"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jtYJoeqKwWoO",
    "outputId": "2acef900-3673-42fa-80a3-daf21d0fc6e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# archive = zipfile.ZipFile('gdrive/My Drive/Microsoft AI/data.zip', 'r')\n",
    "# data = pd.read_csv(archive.open(\"data.tsv\"),delimiter='\\t',header=None)\n",
    "# data.columns = ['queryID','query','response','label','labelID']\n",
    "# data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vl8JsTwB3RZj"
   },
   "outputs": [],
   "source": [
    "# sample = data.iloc[:10000].copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nU8NotLsx7gz",
    "outputId": "c6dd7691-3ce9-41d7-b228-77a13cae7db6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "# split = int(10*((sample.shape[0]*0.7)//10))\n",
    "# print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fILVHTcux7cM"
   },
   "outputs": [],
   "source": [
    "# train = sample[:split]\n",
    "# validation = sample[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vee7XzOyqFE"
   },
   "outputs": [],
   "source": [
    "# train.to_csv(\"gdrive/My Drive/Microsoft AI/DL approach/traindata.tsv\",sep='\\t', index=False,header=None)\n",
    "# validation.to_csv(\"gdrive/My Drive/Microsoft AI/DL approach/validationdata.tsv\",sep='\\t', index=False,header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ybf3W-_Gz6CU"
   },
   "source": [
    "## Text2ctf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T02:47:20.328653Z",
     "start_time": "2018-12-23T02:46:35.890222Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Y2VL2i9YzCdR",
    "outputId": "e4f436bc-1495-41f1-a732-216c517a9277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data conversion is done\n",
      "Validation Data conversion is done\n",
      "Evaluation Data conversion is done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Initialize Global variables \n",
    "GloveEmbeddings = {}\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 50\n",
    "#The following method takes Glove Embedding file and stores all words and their embeddings in a dictionary\n",
    "def loadEmbeddings(embeddingfile):\n",
    "    global GloveEmbeddings,emb_dim\n",
    "\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for line in fe:\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \".join(vec)\n",
    "        GloveEmbeddings[word]=vec\n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    fe.close()\n",
    "\n",
    "\n",
    "def TextDataToCTF(inputfile,outputfile,isEvaluation):\n",
    "    global GloveEmbeddings,emb_dim,max_query_words,max_passage_words\n",
    "\n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")  # Format of the file : query_id \\t query \\t passage \\t label \\t passage_id\n",
    "    fw = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().lower().split(\"\\t\")\n",
    "        query_id,query,passage,label = tokens[0],tokens[1],tokens[2],tokens[3]\n",
    "\n",
    "        #****Query Processing****\n",
    "        words = re.split('\\W+', query)\n",
    "        words = [x for x in words if x] # to remove empty words \n",
    "        word_count = len(words)\n",
    "        remaining = max_query_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_query_words\n",
    "        words = words[:max_query_words] # trim extra words\n",
    "        #create Query Feature vector \n",
    "        query_feature_vector = \"\"\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                query_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "            else:\n",
    "                query_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "        query_feature_vector = query_feature_vector.strip() \n",
    "\n",
    "        #***** Passage Processing **********\n",
    "        words = re.split('\\W+', passage)\n",
    "        words = [x for x in words if x] # to remove empty words \n",
    "        word_count = len(words)\n",
    "        remaining = max_passage_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_passage_words\n",
    "        words = words[:max_passage_words] # trim extra words\n",
    "        #create Passage Feature vector \n",
    "        passage_feature_vector = \"\"\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                passage_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "            else:\n",
    "                passage_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "        passage_feature_vector = passage_feature_vector.strip() \n",
    "\n",
    "        #convert label\n",
    "        label_str = \" 1 0 \" if label==\"0\" else \" 0 1 \" \n",
    "\n",
    "        if(not isEvaluation):\n",
    "            fw.write(\"|qfeatures \"+query_feature_vector+\" |pfeatures \"+passage_feature_vector+\" |labels \"+label_str+\"\\n\")\n",
    "        else:\n",
    "            fw.write(\"|qfeatures \"+query_feature_vector+\" |pfeatures \"+passage_feature_vector+\"|qid \"+str(query_id)+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainFileName = \"traindata.tsv\"\n",
    "    validationFileName = \"validationdata.tsv\"\n",
    "    EvaluationFileName = \"processed_test.tsv\"\n",
    "\n",
    "    embeddingFileName = \"glove.6B.50d.txt\"\n",
    "\n",
    "    loadEmbeddings(embeddingFileName)    \n",
    "\n",
    "    # Convert Query,Passage Text Data to CNTK Text Format(CTF) using 50-Dimension Glove word embeddings \n",
    "    TextDataToCTF(trainFileName,\"TrainData.ctf\",False)\n",
    "    print(\"Train Data conversion is done\")\n",
    "    TextDataToCTF(validationFileName,\"ValidationData.ctf\",False)\n",
    "    print(\"Validation Data conversion is done\")\n",
    "    TextDataToCTF(EvaluationFileName,\"EvaluationData.ctf\",True)\n",
    "    print(\"Evaluation Data conversion is done\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWuWkXed5Qjg"
   },
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T18:45:48.961626Z",
     "start_time": "2018-12-21T18:45:48.841742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-21T18:51:31.017551Z",
     "start_time": "2018-12-21T18:51:30.877019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "DzMSBaR08wsE",
    "outputId": "0bb99b3d-fa12-47a1-b69f-7d652820b3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)\r\n",
      "E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?\r\n"
     ]
    }
   ],
   "source": [
    "#!apt-get install openmpi-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "GDxmDqe98zy4",
    "outputId": "bf8fb36d-e981-4d8a-c0a1-90bcfd6ccf35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cntk-gpu\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/01/6b942ead129ee518e4a08f9a75bbc8a06da9fe25e71d414abbd12fe344b0/cntk_gpu-2.6-cp36-cp36m-manylinux1_x86_64.whl (478.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 478.8MB 27kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from cntk-gpu) (1.14.6)\n",
      "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from cntk-gpu) (1.1.0)\n",
      "Installing collected packages: cntk-gpu\n",
      "Successfully installed cntk-gpu-2.6\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install cntk-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aDlATLuY9cro",
    "outputId": "cba7e706-4b77-4f60-871a-cee3d8bd7f3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mcntk-2.6-cp35-cp35m-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install https://cntk.ai/PythonWheel/CPU-Only/cntk-2.6-cp35-cp35m-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T02:47:30.696420Z",
     "start_time": "2018-12-23T02:47:29.728401Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "-WHrKozQ5v-H",
    "outputId": "1a629023-b8cb-4de9-8633-3a6bb5c69ee2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/cntk/cntk_py_init.py:90: UserWarning: \n",
      "\n",
      "################################################ Missing optional dependency (    MKL     ) ################################################\n",
      "   CNTK may crash if the component that depends on those dependencies is loaded.\n",
      "   Visit https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#mkl for more information.\n",
      "############################################################################################################################################\n",
      "\n",
      "  warnings.warn(WARNING_MSG % ('    MKL     ', 'https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#mkl'))\n",
      "/opt/anaconda/lib/python3.6/site-packages/cntk/cntk_py_init.py:102: UserWarning: \n",
      "\n",
      "################################################ Missing optional dependency (   OpenCV   ) ################################################\n",
      "   CNTK may crash if the component that depends on those dependencies is loaded.\n",
      "   Visit https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#optional-opencv for more information.\n",
      "############################################################################################################################################\n",
      "\n",
      "  warnings.warn(WARNING_MSG % ('   OpenCV   ', 'https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-Linux-Python#optional-opencv'))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cntk as C\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T03:09:51.553359Z",
     "start_time": "2018-12-23T02:47:33.091822Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "crWdDKBU0vVI",
    "outputId": "060928e2-f9d7-4525-e8aa-298d5e0bf265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Vectors are created\n",
      "Training 638 parameters in 14 parameter tensors.\n",
      "\n",
      "Epoch :  0\n",
      "Learning rate per minibatch: 0.03\n",
      "Finished Epoch[1 of 60]: [Training] loss = 1.374198 * 10, metric = 20.00% * 10 7.654s (  1.3 samples/s);\n",
      "Epoch :  1\n",
      "Finished Epoch[2 of 60]: [Training] loss = 1.340756 * 10, metric = 10.00% * 10 18.688s (  0.5 samples/s);\n",
      "Epoch :  2\n",
      "Finished Epoch[3 of 60]: [Training] loss = 1.321612 * 10, metric = 20.00% * 10 18.284s (  0.5 samples/s);\n",
      "Epoch :  3\n",
      "Finished Epoch[4 of 60]: [Training] loss = 1.310757 * 10, metric = 20.00% * 10 18.302s (  0.5 samples/s);\n",
      "Epoch :  4\n",
      "Finished Epoch[5 of 60]: [Training] loss = 1.262159 * 10, metric = 10.00% * 10 18.270s (  0.5 samples/s);\n",
      "Epoch :  5\n",
      "Finished Epoch[6 of 60]: [Training] loss = 1.282151 * 10, metric = 20.00% * 10 18.318s (  0.5 samples/s);\n",
      "Epoch :  6\n",
      "Finished Epoch[7 of 60]: [Training] loss = 1.166369 * 10, metric = 0.00% * 10 18.307s (  0.5 samples/s);\n",
      "Epoch :  7\n",
      "Finished Epoch[8 of 60]: [Training] loss = 1.164882 * 10, metric = 10.00% * 10 18.290s (  0.5 samples/s);\n",
      "Epoch :  8\n",
      "Finished Epoch[9 of 60]: [Training] loss = 1.066926 * 10, metric = 0.00% * 10 18.295s (  0.5 samples/s);\n",
      "Epoch :  9\n",
      "Finished Epoch[10 of 60]: [Training] loss = 1.031662 * 10, metric = 0.00% * 10 18.314s (  0.5 samples/s);\n",
      "Epoch :  10\n",
      "Finished Epoch[11 of 60]: [Training] loss = 1.193706 * 10, metric = 20.00% * 10 18.630s (  0.5 samples/s);\n",
      "Epoch :  11\n",
      "Finished Epoch[12 of 60]: [Training] loss = 0.975147 * 10, metric = 0.00% * 10 18.412s (  0.5 samples/s);\n",
      "Epoch :  12\n",
      "Finished Epoch[13 of 60]: [Training] loss = 1.137318 * 10, metric = 20.00% * 10 18.441s (  0.5 samples/s);\n",
      "Epoch :  13\n",
      "Finished Epoch[14 of 60]: [Training] loss = 1.130354 * 10, metric = 20.00% * 10 18.506s (  0.5 samples/s);\n",
      "Epoch :  14\n",
      "Finished Epoch[15 of 60]: [Training] loss = 1.130469 * 10, metric = 20.00% * 10 18.492s (  0.5 samples/s);\n",
      "Epoch :  15\n",
      "Finished Epoch[16 of 60]: [Training] loss = 1.008715 * 10, metric = 10.00% * 10 18.314s (  0.5 samples/s);\n",
      "Epoch :  16\n",
      "Finished Epoch[17 of 60]: [Training] loss = 1.124273 * 10, metric = 20.00% * 10 18.328s (  0.5 samples/s);\n",
      "Epoch :  17\n",
      "Finished Epoch[18 of 60]: [Training] loss = 0.852112 * 10, metric = 0.00% * 10 18.371s (  0.5 samples/s);\n",
      "Epoch :  18\n",
      "Finished Epoch[19 of 60]: [Training] loss = 1.079099 * 10, metric = 20.00% * 10 18.326s (  0.5 samples/s);\n",
      "Epoch :  19\n",
      "Finished Epoch[20 of 60]: [Training] loss = 0.810901 * 10, metric = 0.00% * 10 18.327s (  0.5 samples/s);\n",
      "Epoch :  20\n",
      "Finished Epoch[21 of 60]: [Training] loss = 0.775245 * 10, metric = 0.00% * 10 18.328s (  0.5 samples/s);\n",
      "Epoch :  21\n",
      "Finished Epoch[22 of 60]: [Training] loss = 0.906476 * 10, metric = 10.00% * 10 18.369s (  0.5 samples/s);\n",
      "Epoch :  22\n",
      "Finished Epoch[23 of 60]: [Training] loss = 0.899899 * 10, metric = 10.00% * 10 18.424s (  0.5 samples/s);\n",
      "Epoch :  23\n",
      "Finished Epoch[24 of 60]: [Training] loss = 0.899501 * 10, metric = 10.00% * 10 18.354s (  0.5 samples/s);\n",
      "Epoch :  24\n",
      "Finished Epoch[25 of 60]: [Training] loss = 0.864654 * 10, metric = 10.00% * 10 18.404s (  0.5 samples/s);\n",
      "Epoch :  25\n",
      "Finished Epoch[26 of 60]: [Training] loss = 0.696128 * 10, metric = 0.00% * 10 18.312s (  0.5 samples/s);\n",
      "Epoch :  26\n",
      "Finished Epoch[27 of 60]: [Training] loss = 1.048075 * 10, metric = 20.00% * 10 18.339s (  0.5 samples/s);\n",
      "Epoch :  27\n",
      "Finished Epoch[28 of 60]: [Training] loss = 1.042222 * 10, metric = 20.00% * 10 18.356s (  0.5 samples/s);\n",
      "Epoch :  28\n",
      "Finished Epoch[29 of 60]: [Training] loss = 0.850469 * 10, metric = 10.00% * 10 18.330s (  0.5 samples/s);\n",
      "Epoch :  29\n",
      "Finished Epoch[30 of 60]: [Training] loss = 0.835513 * 10, metric = 10.00% * 10 18.253s (  0.5 samples/s);\n",
      "Epoch :  30\n",
      "Finished Epoch[31 of 60]: [Training] loss = 0.830695 * 10, metric = 10.00% * 10 18.239s (  0.5 samples/s);\n",
      "Epoch :  31\n",
      "Finished Epoch[32 of 60]: [Training] loss = 1.228889 * 10, metric = 30.00% * 10 18.326s (  0.5 samples/s);\n",
      "Epoch :  32\n",
      "Finished Epoch[33 of 60]: [Training] loss = 0.616733 * 10, metric = 0.00% * 10 18.410s (  0.5 samples/s);\n",
      "Epoch :  33\n",
      "Finished Epoch[34 of 60]: [Training] loss = 0.608793 * 10, metric = 0.00% * 10 18.252s (  0.5 samples/s);\n",
      "Epoch :  34\n",
      "Finished Epoch[35 of 60]: [Training] loss = 1.022031 * 10, metric = 20.00% * 10 18.326s (  0.5 samples/s);\n",
      "Epoch :  35\n",
      "Finished Epoch[36 of 60]: [Training] loss = 0.796032 * 10, metric = 10.00% * 10 18.409s (  0.5 samples/s);\n",
      "Epoch :  36\n",
      "Finished Epoch[37 of 60]: [Training] loss = 0.569155 * 10, metric = 0.00% * 10 18.409s (  0.5 samples/s);\n",
      "Epoch :  37\n",
      "Finished Epoch[38 of 60]: [Training] loss = 0.560188 * 10, metric = 0.00% * 10 18.370s (  0.5 samples/s);\n",
      "Epoch :  38\n",
      "Finished Epoch[39 of 60]: [Training] loss = 1.003925 * 10, metric = 20.00% * 10 18.273s (  0.5 samples/s);\n",
      "Epoch :  39\n",
      "Finished Epoch[40 of 60]: [Training] loss = 0.537598 * 10, metric = 0.00% * 10 18.385s (  0.5 samples/s);\n",
      "Epoch :  40\n",
      "Finished Epoch[41 of 60]: [Training] loss = 0.527937 * 10, metric = 0.00% * 10 18.447s (  0.5 samples/s);\n",
      "Epoch :  41\n",
      "Finished Epoch[42 of 60]: [Training] loss = 1.252139 * 10, metric = 30.00% * 10 18.399s (  0.5 samples/s);\n",
      "Epoch :  42\n",
      "Finished Epoch[43 of 60]: [Training] loss = 0.991977 * 10, metric = 20.00% * 10 18.303s (  0.5 samples/s);\n",
      "Epoch :  43\n",
      "Finished Epoch[44 of 60]: [Training] loss = 0.508987 * 10, metric = 0.00% * 10 18.326s (  0.5 samples/s);\n",
      "Epoch :  44\n",
      "Finished Epoch[45 of 60]: [Training] loss = 1.000595 * 10, metric = 20.00% * 10 18.310s (  0.5 samples/s);\n",
      "Epoch :  45\n",
      "Finished Epoch[46 of 60]: [Training] loss = 0.490502 * 10, metric = 0.00% * 10 18.267s (  0.5 samples/s);\n",
      "Epoch :  46\n",
      "Finished Epoch[47 of 60]: [Training] loss = 0.739433 * 10, metric = 10.00% * 10 18.315s (  0.5 samples/s);\n",
      "Epoch :  47\n",
      "Finished Epoch[48 of 60]: [Training] loss = 0.736693 * 10, metric = 10.00% * 10 18.256s (  0.5 samples/s);\n",
      "Epoch :  48\n",
      "Finished Epoch[49 of 60]: [Training] loss = 0.469985 * 10, metric = 0.00% * 10 18.235s (  0.5 samples/s);\n",
      "Epoch :  49\n",
      "Finished Epoch[50 of 60]: [Training] loss = 0.470703 * 10, metric = 0.00% * 10 18.284s (  0.5 samples/s);\n",
      "Epoch :  50\n",
      "Finished Epoch[51 of 60]: [Training] loss = 0.449666 * 10, metric = 0.00% * 10 18.276s (  0.5 samples/s);\n",
      "Epoch :  51\n",
      "Finished Epoch[52 of 60]: [Training] loss = 1.001578 * 10, metric = 20.00% * 10 18.299s (  0.5 samples/s);\n",
      "Epoch :  52\n",
      "Finished Epoch[53 of 60]: [Training] loss = 0.449745 * 10, metric = 0.00% * 10 18.255s (  0.5 samples/s);\n",
      "Epoch :  53\n",
      "Finished Epoch[54 of 60]: [Training] loss = 0.421606 * 10, metric = 0.00% * 10 18.264s (  0.5 samples/s);\n",
      "Epoch :  54\n",
      "Finished Epoch[55 of 60]: [Training] loss = 0.700745 * 10, metric = 10.00% * 10 18.331s (  0.5 samples/s);\n",
      "Epoch :  55\n",
      "Finished Epoch[56 of 60]: [Training] loss = 1.015953 * 10, metric = 20.00% * 10 18.297s (  0.5 samples/s);\n",
      "Epoch :  56\n",
      "Finished Epoch[57 of 60]: [Training] loss = 1.001816 * 10, metric = 20.00% * 10 18.300s (  0.5 samples/s);\n",
      "Epoch :  57\n",
      "Finished Epoch[58 of 60]: [Training] loss = 1.006092 * 10, metric = 20.00% * 10 18.272s (  0.5 samples/s);\n",
      "Epoch :  58\n",
      "Finished Epoch[59 of 60]: [Training] loss = 0.409807 * 10, metric = 0.00% * 10 18.318s (  0.5 samples/s);\n",
      "Epoch :  59\n",
      "Finished Epoch[60 of 60]: [Training] loss = 0.400552 * 10, metric = 0.00% * 10 18.516s (  0.5 samples/s);\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Initialize Global variables\n",
    "validation_query_vectors = []\n",
    "validation_passage_vectors = []\n",
    "validation_labels = []   \n",
    "q_max_words=12\n",
    "p_max_words=50\n",
    "emb_dim=50\n",
    "\n",
    "## The following LoadValidationSet method reads ctf format validation file and creates query, passage feature vectors and also copies labels for each pair.\n",
    "## the created vectors will be useful to find metrics on validation set after training each epoch which will be useful to decide the best model \n",
    "def LoadValidationSet(validationfile):\n",
    "    f = open(validationfile,'r',encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        y = tokens[3].replace(\"labels\",\"\").strip() # labels\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]\n",
    "        y = [int(w) for w in y.split()]        \n",
    "        y = y[1] # label will be at index 1, i.e. if y = \"1 0\" then label=0 else if y=\"0 1\" then label=1\n",
    "\n",
    "        validation_query_vectors.append(x1)\n",
    "        validation_passage_vectors.append(x2)\n",
    "        validation_labels.append(y)\n",
    "\n",
    "        #print(\"1\")\n",
    "    \n",
    "    print(\"Validation Vectors are created\")\n",
    "\n",
    "#The following method defines a CNN network which has series of convolution and max pooling steps on query features and passage features and then a merge step and it follows a fully connected layer\n",
    "def cnn_network(queryfeatures, passagefeatures, num_classes):\n",
    "    with C.layers.default_options(activation=C.ops.relu, pad=False):\n",
    "        convA1 = C.layers.Convolution2D((3,10),4,pad=False,activation=C.tanh,name='convA1')(queryfeatures) #input : 12*50 #output : 4*10*41\n",
    "        poolA1 = C.layers.MaxPooling((2,3),(2,3),name='poolA1')(convA1)  #output : 4*5*13\n",
    "        convA2 = C.layers.Convolution2D((2,4),2,pad=False,activation=C.tanh,name='convA2')(poolA1) #output : 2*4*10\n",
    "        poolA2 = C.layers.MaxPooling((2,2),(2,2),name='poolA2')(convA2)  #output : 2*2*5\n",
    "        denseA = C.layers.Dense(num_classes*num_classes,activation=C.tanh,name='denseA')(poolA2)  # output : 4\n",
    "         \n",
    "        convB1 = C.layers.Convolution2D((5,10),4,pad=False,activation=C.tanh,name='convB1')(passagefeatures) #input : 50*50  #output : 4*46*41\n",
    "        poolB1 = C.layers.MaxPooling((5,5),(5,5),name='poolB1')(convB1) #output : 4*9*8\n",
    "        convB2 = C.layers.Convolution2D((3,3),2,pad=False,activation=C.tanh,name='convB2')(poolB1) #output : 2*7*6\n",
    "        poolB2 = C.layers.MaxPooling((2,2),(2,2),name='poolB2')(convB2) #output : 2*3*3\n",
    "        denseB = C.layers.Dense(num_classes*num_classes,activation=C.tanh,name='denseB')(poolB2)  # output : 4\n",
    "\n",
    "\n",
    "        mergeQP     = C.element_times(denseA,denseB) # output : 4\n",
    "\n",
    "        model   = C.layers.Dense(num_classes, activation=C.softmax,name=\"overall\")(mergeQP) #outupt : 2 \n",
    "        \n",
    "\n",
    "    return model\n",
    "\n",
    "def create_reader(path, is_training, query_total_dim, passage_total_dim, label_total_dim):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs( queryfeatures = StreamDef(field='qfeatures', shape=query_total_dim,is_sparse=False), \n",
    "                                                            passagefeatures = StreamDef(field='pfeatures', shape=passage_total_dim,is_sparse=False), \n",
    "                                                            labels   = StreamDef(field='labels', shape=label_total_dim,is_sparse=False)\n",
    "                                                            )), \n",
    "                           randomize=is_training, max_sweeps = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "\n",
    "def TrainAndValidate(trainfile):\n",
    "\n",
    "    #*****Hyper-Parameters******\n",
    "    q_max_words= 12\n",
    "    p_max_words = 50\n",
    "    emb_dim = 50\n",
    "    num_classes = 2\n",
    "    minibatch_size = 250\n",
    "    epoch_size = 10 #No.of samples in training set\n",
    "    total_epochs = 60 #Total number of epochs to run\n",
    "    query_total_dim = q_max_words*emb_dim\n",
    "    label_total_dim = num_classes\n",
    "    passage_total_dim = p_max_words*emb_dim\n",
    "\n",
    "\n",
    "    #****** Create placeholders for reading Training Data  ***********\n",
    "    query_input_var =  C.ops.input_variable((1,q_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    passage_input_var =  C.ops.input_variable((1,p_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    output_var = C.input_variable(num_classes,np.float32,is_sparse = False)\n",
    "    train_reader = create_reader(trainfile, True, query_total_dim, passage_total_dim, label_total_dim)\n",
    "    input_map = { query_input_var : train_reader.streams.queryfeatures, passage_input_var:train_reader.streams.passagefeatures, output_var : train_reader.streams.labels}\n",
    "\n",
    "    # ********* Model configuration *******\n",
    "    model_output = cnn_network(query_input_var, passage_input_var, num_classes)\n",
    "    loss = C.binary_cross_entropy(model_output, output_var)\n",
    "    pe = C.classification_error(model_output, output_var)\n",
    "    lr_per_minibatch = C.learning_rate_schedule(0.03, C.UnitType.minibatch)   \n",
    "    learner = C.adagrad(model_output.parameters, lr=lr_per_minibatch)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=total_epochs)\n",
    "\n",
    "    #************Create Trainer with model_output object, learner and loss parameters*************  \n",
    "    trainer = C.Trainer(model_output, (loss, pe), learner, progress_printer)\n",
    "    C.logging.log_number_of_parameters(model_output) ; print()\n",
    "\n",
    "    # **** Train the model in batchwise mode *****\n",
    "    for epoch in range(total_epochs):       # loop over epochs\n",
    "        print(\"Epoch : \",epoch)\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            data = train_reader.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map) # fetch minibatch.\n",
    "            trainer.train_minibatch(data)        # training step\n",
    "            sample_count += data[output_var].num_samples   # count samples processed so far\n",
    "\n",
    "        trainer.summarize_training_progress()\n",
    "                \n",
    "        #model_output.save(\"CNN_{}.dnn\".format(epoch)) # Save the model for every epoch\n",
    "    \n",
    "        #*** Find metrics on validation set after every epoch ******#  (Note : you can skip doing this for every epoch instead to optimize the time, do it after every k epochs)\n",
    "        predicted_labels=[]\n",
    "        for i in range(len(validation_query_vectors)):\n",
    "            queryVec   = np.array(validation_query_vectors[i],dtype=\"float32\").reshape(1,q_max_words,emb_dim)\n",
    "            passageVec = np.array(validation_passage_vectors[i],dtype=\"float32\").reshape(1,p_max_words,emb_dim)\n",
    "            scores = model_output(queryVec,passageVec)[0]   # do forward-prop on model to get score  \n",
    "            predictLabel = 1 if scores[1]>=scores[0] else 0\n",
    "            predicted_labels.append(predictLabel) \n",
    "        metrics = precision_recall_fscore_support(np.array(validation_labels), np.array(predicted_labels), average='binary')\n",
    "        #print(\"precision : \"+str(metrics[0])+\" recall : \"+str(metrics[1])+\" f1 : \"+str(metrics[2])+\"\\n\")\n",
    "        model_output.save(\"CNN.dnn\")\n",
    "\n",
    "\n",
    "    return model_output\n",
    "\n",
    "## The following GetPredictionOnEvalSet method reads all query passage pair vectors from CTF file and does forward prop with trained model to get similarity score\n",
    "## after getting scores for all the pairs, the output will be written into submission file. \n",
    "def GetPredictionOnEvalSet(model,testfile,submissionfile):\n",
    "    global q_max_words,p_max_words,emb_dim\n",
    "\n",
    "    f = open(testfile,'r',encoding=\"utf-8\")\n",
    "    all_scores={} # Dictionary with key = query_id and value = array of scores for respective passages\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        query_id = tokens[3].replace(\"qid\",\"\").strip() # Query_id\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]    \n",
    "        queryVec   = np.array(x1,dtype=\"float32\").reshape(1,q_max_words,emb_dim)\n",
    "        passageVec = np.array(x2,dtype=\"float32\").reshape(1,p_max_words,emb_dim)\n",
    "        score = model(queryVec,passageVec)[0][1] # do forward-prop on model to get score\n",
    "        if(query_id in all_scores):\n",
    "            all_scores[query_id].append(score)\n",
    "        else:\n",
    "            all_scores[query_id] = [score]\n",
    "    fw = open(submissionfile,\"w\",encoding=\"utf-8\")\n",
    "    for query_id in all_scores:\n",
    "        scores = all_scores[query_id]\n",
    "        scores_str = [str(sc) for sc in scores] # convert all scores to string values\n",
    "        scores_str = \"\\t\".join(scores_str) # join all scores in list to make it one string with  tab delimiter.  \n",
    "        fw.write(query_id+\"\\t\"+scores_str+\"\\n\")\n",
    "    fw.close()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainSetFileName = \"TrainData.ctf\"\n",
    "    validationSetFileName = \"ValidationData.ctf\"\n",
    "    testSetFileName = \"EvaluationData.ctf\"\n",
    "    submissionFileName = \"dl_answer.tsv\"\n",
    "   \n",
    "    LoadValidationSet(validationSetFileName)    #Load Validation Query, Passage Vectors from Validation CTF File\n",
    "    model = TrainAndValidate(trainSetFileName) # Training and validation methods    \n",
    "    GetPredictionOnEvalSet(model,testSetFileName,submissionFileName) # Get Predictions on Evaluation Set\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iY8dsUc55oTx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-04-19T19:41:42.826Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tifffile as tif\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from multiprocessing import Pool\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model, save_model\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T17:30:20.604537Z",
     "start_time": "2018-04-18T17:30:20.587495Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, x_metadata, y_metadata, batch_size, crop_size):\n",
    "        self.x = x_metadata\n",
    "        self.y = y_metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.cp = crop_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        return np.array([np.transpose(np.array(tif.imread(file_name), dtype=int)/255.0,\n",
    "                                     (1,2,0))[self.cp:-self.cp,self.cp:-self.cp,:]\n",
    "                        for file_name in batch_x]), np.array(batch_y)         \n",
    "\n",
    "class CNN_Model:\n",
    "    \n",
    "    def __init__(self, directory):\n",
    "        \n",
    "        self.path = directory\n",
    "        self.classes = sorted(list(os.listdir(self.path+\"/train\")))\n",
    "        self.train_metadata_x = []\n",
    "        self.train_metadata_y = []\n",
    "        self.test_metadata_x = []\n",
    "        self.test_metadata_y = []\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            for im in os.listdir(self.path+\"/train/\"+cls):\n",
    "                self.train_metadata_x.append(self.path+\"/train/\"+cls+\"/\"+im)\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            for im in os.listdir(self.path+\"/test/\"+cls):\n",
    "                self.test_metadata_x.append(self.path+\"/test/\"+cls+\"/\"+im)\n",
    "        \n",
    "        np.random.shuffle(self.train_metadata_x)\n",
    "        np.random.shuffle(self.test_metadata_x)\n",
    "        \n",
    "        for p in self.train_metadata_x:\n",
    "            y = np.zeros(10, dtype=int)\n",
    "            y[self.classes.index(p.split(\"/\")[3])] = 1\n",
    "            self.train_metadata_y.append(y)\n",
    "            \n",
    "        for p in self.test_metadata_x:\n",
    "            y = np.zeros(10, dtype=int)\n",
    "            y[self.classes.index(p.split(\"/\")[3])] = 1\n",
    "            self.test_metadata_y.append(y)\n",
    "    \n",
    "    def model_create(self):\n",
    "        \n",
    "        classifier = Sequential()\n",
    "        # Step 1 - Convolution\n",
    "        classifier.add(Conv2D(filters=96, kernel_size=(2, 2), input_shape = (32,32,33), activation = 'relu'))\n",
    "        classifier.add(Conv2D(filters=288, kernel_size=(2, 2), activation = 'relu'))\n",
    "        classifier.add(Conv2D(filters=864, kernel_size=(2, 2), activation = 'relu'))\n",
    "        #classifier.add(Conv2D(filters=, kernel_size=(2, 2), activation = 'relu'))\n",
    "        classifier.add(MaxPool2D(pool_size = (2, 2)))\n",
    "        # Step 3 - Flattening\n",
    "        classifier.add(Flatten())\n",
    "        # Step 4 - Full connection\n",
    "        classifier.add(Dense(128, activation = 'relu'))\n",
    "        classifier.add(Dense(128, activation = 'tanh'))\n",
    "        classifier.add(Dense(10, activation = 'softmax'))\n",
    "        # Compiling the CNN\n",
    "        classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "        classifier.summary()\n",
    "        return classifier\n",
    "    \n",
    "    def fit_generator(self, num_epochs=10, batch_size=32, crop_size=16):        \n",
    "        try:\n",
    "            classifier = load_model(\"Code/Models/CNN_1.h5\")\n",
    "        except:\n",
    "            classifier = self.model_create()\n",
    "            train_data = ImageDataGenerator(self.train_metadata_x, self.train_metadata_y, batch_size, crop_size)\n",
    "            history = classifier.fit_generator(train_data, epochs=num_epochs, use_multiprocessing=True,shuffle=True)\n",
    "            classifier.save(\"Code/Models/CNN_1.h5\")\n",
    "        test_data = ImageDataGenerator(ob.test_metadata_x, ob.test_metadata_y, 64, 16)        \n",
    "        scores = classifier.evaluate_generator(test_data, use_multiprocessing=True)\n",
    "        print(\"Loss : \", scores[0])\n",
    "        print(\"Accuracy : \", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T17:30:21.607616Z",
     "start_time": "2018-04-18T17:30:20.607776Z"
    }
   },
   "outputs": [],
   "source": [
    "ob = CNN_Model(directory=\"Data/Class wise Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-18T17:31:23.584676Z",
     "start_time": "2018-04-18T17:30:21.818141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  0.5859648966405996\n",
      "Accuracy :  0.8179322545669749\n"
     ]
    }
   ],
   "source": [
    "ob.fit_generator(num_epochs=5, batch_size=64, crop_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

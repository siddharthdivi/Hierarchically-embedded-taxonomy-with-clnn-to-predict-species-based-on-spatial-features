{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:01.600449Z",
     "start_time": "2018-05-27T12:29:59.959432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import tifffile as tif\n",
    "import matplotlib.pyplot as plt\n",
    "from multiprocessing import Pool\n",
    "from sklearn.utils import shuffle\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import Sequence\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model, save_model, model_from_json\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "\n",
    "os.chdir(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T05:45:24.580342Z",
     "start_time": "2018-05-27T05:45:23.920373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72849,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ValidationImageGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, x_metadata,batch_size, crop_size):\n",
    "        self.x = x_metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.cp = crop_size\n",
    "        self.dic = {0:[0,120,165,210],1:[35,62,85],2:[7,22,50],3:[0,1,2,3,4],4:[20,60,140],5:[60,100],6:[0,1,2,3,4],7:[1,2,4,8],8:[1,2],9:[0,1,2,3,4]}\n",
    "        self.conv_dic = {0:[133,1176],1:[-10.00984,18.36730],2:[7.846126,20.94560],3:[41.182110,59.95573],4:[302.772980,777.74048],5:[6.182446,36.54550],6:[-28.248663,5.33183],7:[16.744829,41.94211],8:[-14.122952,22.96798],9:[-17.672335,26.44534],10:[-2.738379,26.44534],11:[-17.672335,11.73241],12:[318.297485,2543.30225],13:[43.063732,285.43790],14:[3.022581,135.58406],15:[8.283675,57.78888],16:[121.616867,855.52594],17:[19.868601,421.27750],18:[19.868601,851.60620],19:[60.590000,520.31244],20:[-187.999999,4672.000000]}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "    \n",
    "    def binarization(self,image,un):\n",
    "        img = np.zeros((64,64,len(un)))\n",
    "        for i in range(len(un)):\n",
    "            img[:,:,i] = (image.copy())\n",
    "            img[:,:,i][img[:,:,i] != un[i]] = 0\n",
    "        return img   \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        x = []\n",
    "        for i in range(len(batch_x)):\n",
    "            tempf = tif.imread(batch_x[i])[:21,:,:]\n",
    "            for k in range(21):\n",
    "                tempf[k] = self.conv_dic[k][0] + (self.conv_dic[k][1] - self.conv_dic[k][0]) * ((tempf[k]/255.0) - 0.1) / 0.8\n",
    "            tempf = np.transpose(tempf,(1,2,0))\n",
    "            \n",
    "            l = []\n",
    "            temp = tif.imread(batch_x[i])[21:,:,:]\n",
    "            for k in range(10):\n",
    "                un = np.array(self.dic[k])\n",
    "                un = un[un != 0]\n",
    "                img=np.transpose(self.binarization(temp[k],un),(2,0,1)).tolist()\n",
    "                l.extend(img)\n",
    "            x.append(np.concatenate((tempf,np.transpose(np.array(l),(1,2,0))), axis=2))\n",
    "        return np.array(x)\n",
    "\n",
    "class Val_load():\n",
    "    \n",
    "    def init_load(self, root_dir):\n",
    "        self.path = root_dir\n",
    "    \n",
    "    def validation_data_loading(self,root_dir):\n",
    "        self.x_val=[]\n",
    "        self.patch_ids=[]\n",
    "        for filename in os.listdir(root_dir):\n",
    "            for image in os.listdir(root_dir+'/'+str(filename)):\n",
    "                self.x_val.extend([root_dir+'/'+str(filename)+'/'+str(image)])\n",
    "                self.patch_ids.append(int(image.split(\"_\")[1].split(\".\")[0]))\n",
    "                \n",
    "\n",
    "val_data = Val_load()\n",
    "val_data.validation_data_loading(root_dir='patchTest')\n",
    "val_data.x_val=np.array(val_data.x_val)\n",
    "val_data.x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T04:23:50.088058Z",
     "start_time": "2018-05-27T04:23:23.521230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "classifier = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "classifier.load_weights(\"Code/Models/RCNN_ResNext_Latest.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T05:09:57.217087Z",
     "start_time": "2018-05-27T04:23:50.089983Z"
    }
   },
   "outputs": [],
   "source": [
    "pred=pd.DataFrame()\n",
    "validation_x=ValidationImageGenerator(val_data.x_val,10,16)\n",
    "predictions=classifier.predict_generator(validation_x)\n",
    "pred=pd.concat([pred,pd.DataFrame(predictions[:,-1,:])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T05:09:58.338018Z",
     "start_time": "2018-05-27T05:09:57.219445Z"
    }
   },
   "outputs": [],
   "source": [
    "pred.to_csv(\"TEST-28-MAY-2018.csv\",sep=',',index=False)\n",
    "pkl.dump(val_data.patch_ids, open(\"patch_ids.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T07:02:40.454567Z",
     "start_time": "2018-05-25T06:39:39.465887Z"
    }
   },
   "source": [
    "result = pd.DataFrame()\n",
    "patch_id = np.unique(np.array(pkl.load(open(\"path_id1.pkl\",\"rb\"))))\n",
    "p = pd.read_csv(\"TEST-24-MAY-2018-1.csv\",sep=\",\")\n",
    "dataframe = pd.read_csv(\"occurrences_train.csv\",low_memory=True)\n",
    "datadf = pd.DataFrame(dataframe['species_glc_id'].unique(),columns=[\"0\"])\n",
    "\n",
    "p = p.iloc[:,-3336:]\n",
    "for i in range(p.shape[0]):\n",
    "    if(i%1000 ==0): print(i)\n",
    "    temp = pd.DataFrame()\n",
    "    indices = list(p.values[i].argsort()[-100:])\n",
    "    temp[\"patch_id\"] = [patch_id[i]]*100\n",
    "    temp[\"species_glc_id\"] = datadf.loc[indices].reset_index(drop=True)\n",
    "    temp[\"probability\"] = p.values[i][indices]\n",
    "    temp[\"rank\"] = list(range(1,101))\n",
    "    result = pd.concat([result,temp],axis=0)\n",
    "result.to_csv(\"submission_results1.csv\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:39:54.767050Z",
     "start_time": "2018-05-25T05:07:45.155049Z"
    }
   },
   "source": [
    "pred=pd.DataFrame()\n",
    "for i in range(36000,len(val_data.x_val),1000):\n",
    "    validation_x=ValidationImageGenerator(val_data.x_val[i:i+1000],10,16)\n",
    "    classifier = load_model(\"Code/Models/RCNN_ResNext1.h5\")\n",
    "    print(i)\n",
    "    predictions=classifier.predict_generator(validation_x)\n",
    "    pred=pd.concat([pred,pd.DataFrame(predictions[:,-1,:])], axis=0)\n",
    "pred.to_csv(\"TEST-24-MAY-2018-2.csv\",sep=',',index=False) \n",
    "pkl.dump(patch_id, open(\"path_id2.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:42:22.283434Z",
     "start_time": "2018-05-25T05:40:34.143623Z"
    }
   },
   "source": [
    "p1 = pkl.load(open(\"path_id1.pkl\",\"rb\"))\n",
    "p2 = pkl.load(open(\"path_id2.pkl\",\"rb\"))\n",
    "out1 = pd.read_csv(\"TEST-24-MAY-2018-1.csv\",sep=\",\")\n",
    "out2 = pd.read_csv(\"TEST-24-MAY-2018-2.csv\",sep=\",\")\n",
    "p = p1+p2\n",
    "p = np.unique(np.array(p))\n",
    "out = pd.concat([out1,out2],axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T05:50:20.048497Z",
     "start_time": "2018-05-25T05:50:20.041577Z"
    }
   },
   "source": [
    "p.shape, out.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-25T05:51:53.623Z"
    }
   },
   "source": [
    "pkl.dump(p,open(\"patch_ids.pkl\",\"wb\"))\n",
    "out.to_csv(\"TEST-25-MAY-2018.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T07:34:37.815795Z",
     "start_time": "2018-05-25T07:10:41.001676Z"
    }
   },
   "source": [
    "result = pd.DataFrame()\n",
    "patch_id = np.unique(np.array(pkl.load(open(\"path_id2.pkl\",\"rb\"))))\n",
    "p = pd.read_csv(\"TEST-24-MAY-2018-2.csv\",sep=\",\")\n",
    "dataframe = pd.read_csv(\"occurrences_train.csv\",low_memory=True)\n",
    "datadf = pd.DataFrame(dataframe['species_glc_id'].unique(),columns=[\"0\"])\n",
    "\n",
    "p = p.iloc[:,-3336:]\n",
    "for i in range(p.shape[0]):\n",
    "    if(i%1000 ==0): print(i)\n",
    "    temp = pd.DataFrame()\n",
    "    indices = list(p.values[i].argsort()[-100:])\n",
    "    temp[\"patch_id\"] = [patch_id[i]]*100\n",
    "    temp[\"species_glc_id\"] = datadf.loc[indices].reset_index(drop=True)\n",
    "    temp[\"probability\"] = p.values[i][indices]\n",
    "    temp[\"rank\"] = list(range(1,101))\n",
    "    result = pd.concat([result,temp],axis=0)\n",
    "result.to_csv(\"submission_results2.csv\",sep=\";\",index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:08.738861Z",
     "start_time": "2018-05-25T08:36:08.214888Z"
    }
   },
   "source": [
    "res1_first.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:12.369985Z",
     "start_time": "2018-05-25T08:36:12.358347Z"
    }
   },
   "source": [
    "np.array(res1.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:14.381685Z",
     "start_time": "2018-05-25T08:36:14.376970Z"
    }
   },
   "source": [
    "res1_first = np.array([[1,2852,0.0014192296657711267,1]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:15.421628Z",
     "start_time": "2018-05-25T08:36:15.318459Z"
    }
   },
   "source": [
    "res1_first = np.array([[1,2852,0.0014192296657711267,1]])\n",
    "res1_second = res1.values[:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:16.955282Z",
     "start_time": "2018-05-25T08:36:16.943511Z"
    }
   },
   "source": [
    "np.array(res1_first).shape , res1_second.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:17.856148Z",
     "start_time": "2018-05-25T08:36:17.710008Z"
    }
   },
   "source": [
    "new_res1 = np.vstack((res1_first,res1_second))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:18.527159Z",
     "start_time": "2018-05-25T08:36:18.380581Z"
    }
   },
   "source": [
    "res2_first = np.array([[10000,143,0.0014993970980867743,1]])\n",
    "res2_second = res2.values[:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:19.143251Z",
     "start_time": "2018-05-25T08:36:19.137352Z"
    }
   },
   "source": [
    "res2_first.shape , res2_second.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:20.165676Z",
     "start_time": "2018-05-25T08:36:20.067020Z"
    }
   },
   "source": [
    "new_res2 = np.vstack((res2_first,res2_second))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:36:20.866201Z",
     "start_time": "2018-05-25T08:36:20.681313Z"
    }
   },
   "source": [
    "final_res = np.vstack((new_res1,new_res2))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:39:11.318328Z",
     "start_time": "2018-05-25T08:38:11.750118Z"
    }
   },
   "source": [
    "#final_df = pd.DataFrame(final_res,columns=[\"Patch_ID\",\"species\",\"prob\",\"rank\"])\n",
    "final_df.to_csv(\"SSN_CS_19_run100.csv\",sep=\";\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:37:52.218184Z",
     "start_time": "2018-05-25T08:37:51.587808Z"
    }
   },
   "source": [
    "final_df['Patch_ID'] = final_df['Patch_ID'].astype(int)\n",
    "final_df['species'] = final_df['species'].astype(int)\n",
    "final_df['rank'] = final_df['rank'].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:38:00.643304Z",
     "start_time": "2018-05-25T08:38:00.634919Z"
    }
   },
   "source": [
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T08:35:59.906962Z",
     "start_time": "2018-05-25T08:35:53.948719Z"
    }
   },
   "source": [
    "res1 = pd.read_csv(\"submission_results1.csv\",sep=\";\")\n",
    "res2 = pd.read_csv(\"submission_results2.csv\",sep=\";\")\n",
    "#res = pd.concat([res1,res2],ignore_index=True,axis=0)\n",
    "#res.to_csv(\"SSN_CS_19_run100.csv\",sep=\";\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-25T07:50:47.917831Z",
     "start_time": "2018-05-25T07:50:47.910456Z"
    }
   },
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:54:18.489338Z",
     "start_time": "2018-05-06T23:54:17.463703Z"
    }
   },
   "source": [
    "import pickle as pkl\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "def hyperbolic_distances(z,w):\n",
    "    return np.average(np.arccosh(1 + (2 * (np.abs(z-w)**2)) / ((1-np.abs(z)**2)*(1-np.abs(w)**2))))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def prediction_to_species(p, max_ranks=5):\n",
    "    \"\"\"\n",
    "    Function that will take in the prediction of the CNN-LSTM model and get the species id.\n",
    "\n",
    "    Inputs :\n",
    "    ypred : Vector of dimensions n x 10.\n",
    "\n",
    "    Outputs :\n",
    "    DataFrame of type :\n",
    "    patch_id (int), species_glc_id(int) , probability(float) , rank(int) .\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    e2 = pkl.load(open(\"Data/final_embeddings.pkl\", \"rb\"))\n",
    "    #print (\"Size of the species embedding pickle : \", e2.shape)\n",
    "    ypred = p.values[:,2:]\n",
    "    # Species embeddings from indices 0 to 3335, of shape 3336.\n",
    "    species_embedding = e2['species_glc_id'][-3336:, :]\n",
    "    df = pd.read_csv(\"occurrences_train.csv\", low_memory=False)\n",
    "    df = df[[\"class\", \"order\", \"family\", \"genus\", \"species_glc_id\"]]\n",
    "    unique_species_id = df['species_glc_id'].unique()\n",
    "    masterdf = pd.DataFrame()\n",
    "\n",
    "    cntr = 0\n",
    "    for species in ypred:\n",
    "        \n",
    "        if(cntr % 1000 == 0):\n",
    "            print(cntr)\n",
    "        # Now compare this vector with each vector in species_embedding.\n",
    "        '''\n",
    "        GET THE PATCH FROM THE NAME OF THE IMAGES.\n",
    "        patch_id is a list of patch names for the test images.\n",
    "        '''\n",
    "        patch_id = np.array([p.values[cntr,1]]*max_ranks).reshape(-1,1)\n",
    "        cntr+=1\n",
    "        embedding_distances = euclidean_distances(species_embedding, species.reshape(-1,10))\n",
    "        top_indices = embedding_distances.flatten().argsort()[:max_ranks]\n",
    "        top_distances = embedding_distances[top_indices].tolist()\n",
    "        top_distances = softmax(np.array([1.0/i[0] for i in top_distances])).reshape(-1,1)\n",
    "        rank = np.array(range(1,len(top_indices)+1)).reshape(-1,1)\n",
    "        # Now get the species_glc_id , from the numpy array of unique species.\n",
    "\n",
    "        # Use the top_indices, to get the species_glc_id from unique_species_id.\n",
    "        species_glc_id = unique_species_id[top_indices].reshape(-1,1)\n",
    "        masterdf = pd.concat([masterdf, pd.DataFrame(\n",
    "            np.concatenate((patch_id, species_glc_id, top_distances, rank), axis=1))], axis=0)\n",
    "\n",
    "    #Writing the dataframe to a csv.\n",
    "    masterdf.columns = [\"patch_id\",\"species_glc_id\",\"probability\",\"rank\"]\n",
    "    masterdf[\"patch_id\"] = masterdf[\"patch_id\"].astype(int)\n",
    "    masterdf[\"species_glc_id\"] = masterdf[\"species_glc_id\"].astype(int)\n",
    "    masterdf[\"probability\"] = masterdf[\"probability\"].astype(float)\n",
    "    masterdf[\"rank\"] = masterdf[\"rank\"].astype(int)\n",
    "    return masterdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:03.171877Z",
     "start_time": "2018-05-27T12:30:03.061768Z"
    }
   },
   "outputs": [],
   "source": [
    "t = pkl.load(open(\"Data/final_embeddings.pkl\",\"rb\"),encoding='latin')\n",
    "e = []\n",
    "for i in range(1,3337):\n",
    "    e.append(t[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:04.340043Z",
     "start_time": "2018-05-27T12:30:04.178386Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = pd.read_csv(\"TEST-28-MAY-2018.csv\")\n",
    "patch_ids = pkl.load(open(\"patch_ids.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:05.111754Z",
     "start_time": "2018-05-27T12:30:05.032565Z"
    }
   },
   "outputs": [],
   "source": [
    "def hyperbolic_distances(z,w):\n",
    "    return np.average(np.arccosh(1 + (2 * (np.abs(z-w)**2)) / ((1-np.abs(z)**2)*(1-np.abs(w)**2))))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "def embeddings_to_prob(pred, patch_ids, embeddings, max_ranks):\n",
    "    master_df = pd.DataFrame(columns=['patch_id','species_id','prob','rank']) \n",
    "    patches = np.array([patch_ids]*max_ranks).reshape((-1,1))\n",
    "    dist_indices = []\n",
    "    for j in range(3336):\n",
    "        dist_indices.append(hyperbolic_distances(pred,e[j]))\n",
    "    dist_indices = np.array(dist_indices)\n",
    "    min_indices = dist_indices.argsort()[:max_ranks]\n",
    "    probs = softmax(1.0/dist_indices[min_indices.tolist()]).reshape((-1,1))\n",
    "    species_ids = np.array(min_indices+1.0).reshape((-1,1))\n",
    "    ranks = np.array(list(range(1,max_ranks+1))).reshape((-1,1))\n",
    "    master_df = pd.concat([master_df,pd.DataFrame(np.concatenate((patches,species_ids,probs,ranks),axis=1),columns=['patch_id','species_id','prob','rank'])],axis=0)\n",
    "    master_df['patch_id'] = master_df['patch_id'].astype(int)\n",
    "    master_df['species_id'] = master_df['species_id'].astype(int)\n",
    "    master_df['rank'] = master_df['rank'].astype(int)\n",
    "#         print(master_df.head())|\n",
    "    return master_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:08.455455Z",
     "start_time": "2018-05-27T12:30:06.590501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many ranks for each image : 100\n"
     ]
    }
   ],
   "source": [
    "num_ranks = int(input(\"How many ranks for each image : \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:12.338881Z",
     "start_time": "2018-05-27T12:30:11.592791Z"
    }
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(0,pred.shape[0]):\n",
    "    l.append((pred.values[i], patch_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:30:12.812834Z",
     "start_time": "2018-05-27T12:30:12.807795Z"
    }
   },
   "outputs": [],
   "source": [
    "def f(l1):\n",
    "    return embeddings_to_prob(l1[0],l1[1],e,num_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T12:50:18.545745Z",
     "start_time": "2018-05-27T12:30:15.505327Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "p = Pool(14)\n",
    "res = p.map(f,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T13:08:54.478724Z",
     "start_time": "2018-05-27T13:08:15.484456Z"
    }
   },
   "outputs": [],
   "source": [
    "mpd = pd.concat(res,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-27T13:10:40.563015Z",
     "start_time": "2018-05-27T13:10:05.734333Z"
    }
   },
   "outputs": [],
   "source": [
    "mpd.to_csv(\"SSN_CS_19_run4.csv\",sep=\";\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-06T23:57:53.022541Z",
     "start_time": "2018-05-06T23:56:30.268431Z"
    }
   },
   "outputs": [],
   "source": [
    "mpd = pd.DataFrame()\n",
    "for i in range(0,p.shape[0],20000):\n",
    "    print(\"Main Iter : \", i/20000)\n",
    "    if(i==0):\n",
    "        mpd = prediction_to_species(p, num_ranks)\n",
    "    else:\n",
    "        mpd = pd.concat([mpd, prediction_to_species(p, num_ranks)], axis=0)\n",
    "mpd.to_csv(\"MLRG_SSN3_run100.csv\",sep=\";\",index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
